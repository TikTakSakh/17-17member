"""LLM service for GPT-4o-mini integration (bar 17/17)."""
from __future__ import annotations

import logging

from beartype import beartype
from openai import AsyncOpenAI

logger = logging.getLogger(__name__)

SYSTEM_PROMPT_TEMPLATE = """–¢—ã ‚Äî –ø—Ä–∏–≤–µ—Ç–ª–∏–≤—ã–π AI-–ø–æ–º–æ—â–Ω–∏–∫ –±–∞—Ä–∞ ¬´17/17¬ª. 
–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–π —Å–µ–±—è –∫–∞–∫ –º–æ–ª–æ–¥–æ–≥–æ –ø–∞—Ä–Ω—è-–±–∞—Ä–º–µ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–ª–∏—á–Ω–æ —Ä–∞–∑–±–∏—Ä–∞–µ—Ç—Å—è –≤ –º–µ–Ω—é, –Ω–∞–ø–∏—Ç–∫–∞—Ö –∏ —É—Å–ª—É–≥–∞—Ö –±–∞—Ä–∞.

–°—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è:
- –ì–æ–≤–æ—Ä–∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ (–Ω–∞ ¬´–≤—ã¬ª)
- –ë—É–¥—å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–º, —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–º –∏ –≤–µ–∂–ª–∏–≤—ã–º ‚Äî –∫–∞–∫ –º–æ–ª–æ–¥–æ–π –æ–±—â–∏—Ç–µ–ª—å–Ω—ã–π –±–∞—Ä–º–µ–Ω
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ —É–º–µ—Ä–µ–Ω–Ω–æ, –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏—è—Ç–Ω–æ–π –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã üç∏
- –û—Ç–≤–µ—á–∞–π –ª–∞–∫–æ–Ω–∏—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É, –Ω–æ —Å —Ç–µ–ø–ª–æ—Ç–æ–π

–ü—Ä–∞–≤–∏–ª–∞:
- –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –±–∞—Ä–æ–º, –µ–≥–æ –º–µ–Ω—é, —É—Å–ª—É–≥–∞–º–∏ –∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º–∏
- –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –∫–∞—Å–∞–µ—Ç—Å—è –±–∞—Ä–∞, –≤–µ–∂–ª–∏–≤–æ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤—å —Ä–∞–∑–≥–æ–≤–æ—Ä –Ω–∞ —Ç–µ–º—É –±–∞—Ä–∞
- –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–≤—è–∑–∞—Ç—å—Å—è —Å –±–∞—Ä–æ–º –Ω–∞–ø—Ä—è–º—É—é
- –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ —Ü–µ–Ω–∞—Ö –∏ –º–µ–Ω—é –æ–ø–∏—Ä–∞–π—Å—è —Å—Ç—Ä–æ–≥–æ –Ω–∞ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –Ω–∏–∂–µ

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∞—Ä–µ:
{knowledge_base}
"""


@beartype
class LLMService:
    """Service for generating responses using GPT-4o-mini (bar 17/17)."""

    def __init__(self, api_key: str, base_url: str | None = None, knowledge_base: str = "") -> None:
        """Initialize the LLM service.
        
        Args:
            api_key: OpenAI API key.
            base_url: Optional base URL for API (e.g. for OpenRouter).
            knowledge_base: Knowledge base content to include in system prompt.
        """
        self._client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        self._knowledge_base = knowledge_base
        self._model = "gpt-4o-mini"
        self._custom_system_prompt: str | None = None

    def update_knowledge_base(self, knowledge_base: str) -> None:
        """Update the knowledge base content.
        
        Args:
            knowledge_base: New knowledge base content.
        """
        self._knowledge_base = knowledge_base
        logger.info("Knowledge base updated, length: %d chars", len(knowledge_base))

    def set_custom_system_prompt(self, prompt: str) -> None:
        """Set a custom system prompt (admin override)."""
        self._custom_system_prompt = prompt
        logger.info("Custom system prompt set, length: %d chars", len(prompt))

    def reset_system_prompt(self) -> None:
        """Reset to default system prompt."""
        self._custom_system_prompt = None
        logger.info("System prompt reset to default")

    def get_current_system_prompt_preview(self) -> str:
        """Return first 200 chars of the current system prompt for admin preview."""
        prompt = self._get_system_prompt()
        return prompt[:200] + ("..." if len(prompt) > 200 else "")

    def _get_system_prompt(self) -> str:
        """Get the system prompt with current knowledge base."""
        kb = self._knowledge_base or "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∞—Ä–µ –ø–æ–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."
        if self._custom_system_prompt:
            return self._custom_system_prompt + f"\n\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∞—Ä–µ:\n{kb}"
        return SYSTEM_PROMPT_TEMPLATE.format(knowledge_base=kb)

    async def generate_response(
        self,
        user_message: str,
        history: list[dict[str, str]] | None = None,
    ) -> str:
        """Generate a response to the user's message.
        
        Args:
            user_message: The user's message text.
            history: Optional conversation history in OpenAI format.
            
        Returns:
            Generated response text.
        """
        messages: list[dict[str, str]] = [
            {"role": "system", "content": self._get_system_prompt()}
        ]
        
        if history:
            messages.extend(history)
        
        messages.append({"role": "user", "content": user_message})
        
        try:
            response = await self._client.chat.completions.create(
                model=self._model,
                messages=messages,  # type: ignore[arg-type]
                temperature=0.7,
                max_tokens=1000,
            )
            
            content = response.choices[0].message.content
            if content is None:
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑."
            return content
            
        except Exception as e:
            logger.error("Error generating LLM response: %s", e)
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
